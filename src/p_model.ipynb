{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from Chord import Chord\n",
    "from Rule import Rule\n",
    "from collections import defaultdict\n",
    "import pitchtypes as pt\n",
    "from probabilistic_model import ProbabilisticModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = ProbabilisticModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('../eda/JazzHarmonyTreebank/treebank.json')\n",
    "trees_data = data[data['trees'].notna()]['trees']\n",
    "ccts = [x[0]['complete_constituent_tree'] for x in list(trees_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model.fit(ccts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.4410143329658214,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('sus', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0033076074972436605,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('sus', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.14285714285714285,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('minor', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.09523809523809523,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.3263506063947078,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.07003089598352215,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.25060075523515274,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.1826295914864401,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0624785444558874,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.006615214994487321,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.002205071664829107,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.29694473051836595,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.05181918412348401,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.03364229316855476,\n",
       " (('child_intervals', ('8', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0011025358324145535,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.006865774116031583,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.013731548232063165,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.05733186328555678,\n",
       " (('child_intervals', ('9', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.005149330587023687,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.013044970820460007,\n",
       " (('child_intervals', ('9', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.003089598352214212,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.004119464469618949,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.004119464469618949,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.007717750826901874,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.014332965821389196,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('sus', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0017164435290078957,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.007717750826901874,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.1206896551724138,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'major')): 0.0027463096464126332,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.004119464469618949,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.002403020940611054,\n",
       " (('child_intervals', ('2', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0033076074972436605,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('sus', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0020597322348094747,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('sus', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.2857142857142857,\n",
       " (('child_intervals', ('2', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.002403020940611054,\n",
       " (('child_intervals', ('0', '9')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'major')): 0.0054926192928252664,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.005149330587023687,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.025358324145534728,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('unknown', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0011025358324145535,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0020597322348094747,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('sus', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('minor', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.047619047619047616,\n",
       " (('child_intervals', ('0', '3')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'minor')): 0.009922822491730982,\n",
       " (('child_intervals', ('0', '3')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.002205071664829107,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('unknown', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.008820286659316428,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('major', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.047619047619047616,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.002403020940611054,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0013731548232063166,\n",
       " (('child_intervals', ('0', '3')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('0', '6')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.007717750826901874,\n",
       " (('child_intervals', ('0', '9')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('0', '1')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'major')): 0.0037761757638173706,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0011025358324145535,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.002205071664829107,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('major', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.09523809523809523,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0017164435290078957,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('sus', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.047619047619047616,\n",
       " (('child_intervals', ('8', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('9', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.002205071664829107,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('minor', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.09523809523809523,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('major', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.09523809523809523,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('unknown', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.015435501653803748,\n",
       " (('child_intervals', ('8', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.006865774116031583,\n",
       " (('child_intervals', ('9', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.034482758620689655,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('unknown', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.1206896551724138,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.08620689655172414,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.034482758620689655,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.1896551724137931,\n",
       " (('child_intervals', ('8', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0017164435290078957,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.017241379310344827,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0010298661174047373,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.034482758620689655,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('2', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.1206896551724138,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.034482758620689655,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.13793103448275862,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.05172413793103448,\n",
       " (('child_intervals', ('0', '1')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'minor')): 0.0011025358324145535,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('unknown', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.047619047619047616,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.017241379310344827,\n",
       " (('child_intervals', ('0', '1')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_model.prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_subtree(subtree):\n",
    "    if isinstance(subtree, dict):\n",
    "        label = Chord(subtree['label'])\n",
    "        child_intervals = [str(label.distance_to(Chord(x['label']))) for x in subtree['children']]\n",
    "        child_qualities = [Chord(x['label']).quality for x in subtree['children']]\n",
    "        rule = Rule(label.quality, child_intervals, child_qualities)\n",
    "        \n",
    "        return rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashable(d):\n",
    "    def convert(value):\n",
    "        if isinstance(value, dict):\n",
    "            return tuple(sorted((k, convert(v)) for k, v in value.items()))\n",
    "        elif isinstance(value, list):\n",
    "            return tuple(convert(v) for v in value)\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    return tuple(sorted((k, convert(v)) for k, v in d.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = defaultdict(int)\n",
    "lhs_count_dict = defaultdict(int)\n",
    "\n",
    "def parse_leftmost(tree):\n",
    "    output = []\n",
    "    steps = []\n",
    "    stack = [tree]\n",
    "    parsed_so_far = []\n",
    "\n",
    "    while stack:\n",
    "        current = stack.pop(0)\n",
    "        rule = parse_subtree(current)\n",
    "        rule_key = rule.make_hashable()\n",
    "        count_dict[rule_key] += 1\n",
    "        lhs_count_dict[rule.lhs()] += 1\n",
    "\n",
    "        steps.append({\n",
    "            'current': current['label'],\n",
    "            'parsed_before': parsed_so_far.copy()\n",
    "        })\n",
    "\n",
    "        if current.get('children'):\n",
    "            stack = current['children'] + stack\n",
    "        else:\n",
    "            output.append(current['label'])\n",
    "\n",
    "        parsed_so_far.append(current['label'])\n",
    "\n",
    "    return steps, count_dict, lhs_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cct in ccts:\n",
    "    steps, count_dict, lhs_count_dict = parse_leftmost(cct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(count_dict.items(), key=lambda x: x[1], reverse=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sus': 47, 'unknown': 321, 'minor': 2125, 'major': 5455}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs_count_dict = dict(sorted(lhs_count_dict.items(), key=lambda x: x[1], reverse=False))\n",
    "lhs_count_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_probs(count_dict, lhs_count_dict):\n",
    "    prob_dict = {}\n",
    "    for rule, count in count_dict.items():\n",
    "        lhs = rule[2][1]\n",
    "        if lhs_count_dict[lhs] == 0:\n",
    "            print(lhs)\n",
    "        prob_dict[rule] = count / lhs_count_dict[lhs]\n",
    "    return prob_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_probability(node, prob_dict):\n",
    "    subtree = parse_subtree(node)\n",
    "    rule_key = subtree.make_hashable()\n",
    "    return prob_dict.get(rule_key, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tree_probability(tree, prob_dict):\n",
    "    if not tree.get('children'):\n",
    "        return 1.0  \n",
    "\n",
    "    prob = get_rule_probability(tree, prob_dict)\n",
    "\n",
    "    for child in tree['children']:\n",
    "        prob *= compute_tree_probability(child, prob_dict)\n",
    "\n",
    "    return prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cct in ccts:\n",
    "    steps, count_dict, lhs_count_dict = parse_leftmost(cct)\n",
    "    prob_dict = compute_conditional_probs(count_dict, lhs_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8069074877778802e-11"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_tree_probability(ccts[3], expanded_prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dict = p_model.prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.4410143329658214,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('sus', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0033076074972436605,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('sus', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.14285714285714285,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('minor', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.09523809523809523,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.3263506063947078,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.07003089598352215,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.25060075523515274,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.1826295914864401,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0624785444558874,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.006615214994487321,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.002205071664829107,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.29694473051836595,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.05181918412348401,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.03364229316855476,\n",
       " (('child_intervals', ('8', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0011025358324145535,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.006865774116031583,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.013731548232063165,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.05733186328555678,\n",
       " (('child_intervals', ('9', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.005149330587023687,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.013044970820460007,\n",
       " (('child_intervals', ('9', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.003089598352214212,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.004119464469618949,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.004119464469618949,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.007717750826901874,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.014332965821389196,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('sus', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0017164435290078957,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.007717750826901874,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.1206896551724138,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'major')): 0.0027463096464126332,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.004119464469618949,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.002403020940611054,\n",
       " (('child_intervals', ('2', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0033076074972436605,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('sus', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0020597322348094747,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('sus', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.2857142857142857,\n",
       " (('child_intervals', ('2', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.002403020940611054,\n",
       " (('child_intervals', ('0', '9')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'major')): 0.0054926192928252664,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.005149330587023687,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.025358324145534728,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('unknown', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0011025358324145535,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0020597322348094747,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('sus', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('minor', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.047619047619047616,\n",
       " (('child_intervals', ('0', '3')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'minor')): 0.009922822491730982,\n",
       " (('child_intervals', ('0', '3')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.002205071664829107,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('unknown', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.008820286659316428,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('major', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.047619047619047616,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.002403020940611054,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0013731548232063166,\n",
       " (('child_intervals', ('0', '3')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('0', '6')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.007717750826901874,\n",
       " (('child_intervals', ('0', '9')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('0', '1')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'major')): 0.0037761757638173706,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.0011025358324145535,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('major', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.002205071664829107,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('major', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.09523809523809523,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0017164435290078957,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('sus', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.047619047619047616,\n",
       " (('child_intervals', ('8', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('9', '0')),\n",
       "  ('child_qualities', ('minor', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.002205071664829107,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('minor', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.09523809523809523,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('major', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.09523809523809523,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('unknown', 'minor')),\n",
       "  ('parent_quality', 'minor')): 0.015435501653803748,\n",
       " (('child_intervals', ('8', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.006865774116031583,\n",
       " (('child_intervals', ('9', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.034482758620689655,\n",
       " (('child_intervals', ('0', '0')),\n",
       "  ('child_qualities', ('unknown', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.1206896551724138,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.08620689655172414,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.034482758620689655,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.1896551724137931,\n",
       " (('child_intervals', ('8', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0017164435290078957,\n",
       " (('child_intervals', ('3', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.017241379310344827,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0010298661174047373,\n",
       " (('child_intervals', ('5', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.034482758620689655,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.0006865774116031583,\n",
       " (('child_intervals', ('4', '0')),\n",
       "  ('child_qualities', ('unknown', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('2', '0')),\n",
       "  ('child_qualities', ('minor', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915,\n",
       " (('child_intervals', ('10', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.1206896551724138,\n",
       " (('child_intervals', ('11', '0')),\n",
       "  ('child_qualities', ('major', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.034482758620689655,\n",
       " (('child_intervals', ('1', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.13793103448275862,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.05172413793103448,\n",
       " (('child_intervals', ('0', '1')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'minor')): 0.0011025358324145535,\n",
       " (('child_intervals', ('7', '0')),\n",
       "  ('child_qualities', ('unknown', 'sus')),\n",
       "  ('parent_quality', 'sus')): 0.047619047619047616,\n",
       " (('child_intervals', ('6', '0')),\n",
       "  ('child_qualities', ('minor', 'unknown')),\n",
       "  ('parent_quality', 'unknown')): 0.017241379310344827,\n",
       " (('child_intervals', ('0', '1')),\n",
       "  ('child_qualities', ('major', 'major')),\n",
       "  ('parent_quality', 'major')): 0.00034328870580157915}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_prob_dict(prob_dict, default_prob=1e-6):\n",
    "    INTERVAL_VOCAB = list(str(i) for i in range(0,12))\n",
    "    QUALITY_VOCAB = ['major', 'minor', 'sus', 'unknown']\n",
    "\n",
    "    all_child_intervals = list(product(INTERVAL_VOCAB, repeat=2))\n",
    "    all_child_qualities = list(product(QUALITY_VOCAB, repeat=2))\n",
    "    all_parent_qualities = QUALITY_VOCAB\n",
    "\n",
    "    count_added = 0\n",
    "\n",
    "    for interval_combo, quality_combo, parent_quality in product(all_child_intervals, all_child_qualities, all_parent_qualities):\n",
    "        key = Rule(parent_quality, interval_combo, quality_combo).make_hashable()\n",
    "\n",
    "        if key not in prob_dict:\n",
    "            prob_dict[key] = default_prob\n",
    "            count_added += 1\n",
    "\n",
    "    print(f\"Added {count_added} new rules to prob_dict.\")\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('child_intervals', ('0', '4')),\n",
       " ('child_qualities', ('major', 'minor')),\n",
       " ('parent_quality', 'major'))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rule(\n",
    "    'major',\n",
    "    ['0', '4'],\n",
    "    ['major', 'minor']\n",
    ").make_hashable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing combinations: 0\n"
     ]
    }
   ],
   "source": [
    "INTERVAL_VOCAB = [str(i) for i in range(12)]\n",
    "QUALITY_VOCAB = ['major', 'minor', 'sus', 'unknown']\n",
    "\n",
    "missing = []\n",
    "\n",
    "for parent_quality in QUALITY_VOCAB:\n",
    "    for int1 in INTERVAL_VOCAB:\n",
    "        for q1 in QUALITY_VOCAB:\n",
    "            for int2 in INTERVAL_VOCAB:\n",
    "                for q2 in QUALITY_VOCAB:\n",
    "                    key = Rule(parent_quality, [int1, int2], [q1, q2]).make_hashable()\n",
    "                    if key not in prob_dict:\n",
    "                        missing.append(key)\n",
    "\n",
    "print(f\"Total missing combinations: {len(missing)}\")\n",
    "for m in missing[:10]:  # just show a few\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 new rules to prob_dict.\n"
     ]
    }
   ],
   "source": [
    "prob_dict = expand_prob_dict(prob_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [Rule.unhash(i) for i in prob_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, chord):\n",
    "        self.chord = chord\n",
    "        self.left = None           # child node\n",
    "        self.right = None          # child node\n",
    "        self.parent = None         # to be filled in later\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TreeNode(label={self.chord.label})\"\n",
    "\n",
    "\n",
    "def create_parent_node(left, right, parent, parent_quality):\n",
    "    parent = TreeNode(Chord(f\"{parent.chord.label}\"))\n",
    "    parent.left = left\n",
    "    parent.right = right\n",
    "    left.parent = parent\n",
    "    right.parent = parent\n",
    "\n",
    "    return parent\n",
    "\n",
    "def get_top_level_nodes(nodes):\n",
    "    \"\"\"\n",
    "    Given a list of TreeNodes (leaves and partial trees), \n",
    "    return the current top-level (root) nodes that are not children of any other node.\n",
    "    \"\"\"\n",
    "    return [node for node in nodes if node.parent is None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_intervals(chord1,chord2):\n",
    "\n",
    "    intervals = []\n",
    "    intervals.append([str(chord1.distance_to(chord1)), str(chord1.distance_to(chord2))])\n",
    "    intervals.append([str(chord1.distance_from(chord2)), str(chord2.distance_to(chord2))])\n",
    "\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualities(chord1,chord2):\n",
    "    return [chord1.quality, chord2.quality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_rhs_from_str(chord1,chord2):\n",
    "    return [(get_possible_intervals(chord1, chord2)[0], get_qualities(chord1, chord2)), (get_possible_intervals(chord1, chord2)[1], get_qualities(chord1, chord2)),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_applicable_rules(rhs_list, rules):\n",
    "    applicable_rules = []\n",
    "    for rule in rules:\n",
    "        for rhs in rhs_list:\n",
    "            if rule.rhs() == rhs:\n",
    "                applicable_rules.append(rule)\n",
    "    return applicable_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, history_dim, action_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.history_encoder = nn.Sequential(\n",
    "            nn.Linear(history_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.action_encoder = nn.Sequential(\n",
    "            nn.Linear(action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Output one Q-value\n",
    "        )\n",
    "\n",
    "    def forward(self, history_vec, actions_vecs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            history_vec: Tensor (batch_size, history_dim)\n",
    "            actions_vecs: Tensor (batch_size, num_actions, action_dim)\n",
    "\n",
    "        Returns:\n",
    "            q_values: Tensor (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        batch_size, num_actions, action_dim = actions_vecs.shape\n",
    "\n",
    "        # Encode the history once\n",
    "        history_encoded = self.history_encoder(history_vec)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Flatten actions and encode them\n",
    "        actions_flat = actions_vecs.view(-1, action_dim)  # (batch_size * num_actions, action_dim)\n",
    "        actions_encoded = self.action_encoder(actions_flat)  # (batch_size * num_actions, hidden_dim)\n",
    "        actions_encoded = actions_encoded.view(batch_size, num_actions, -1)  # (batch_size, num_actions, hidden_dim)\n",
    "\n",
    "        # Repeat history for each action\n",
    "        history_repeated = history_encoded.unsqueeze(1).repeat(1, num_actions, 1)  # (batch_size, num_actions, hidden_dim)\n",
    "\n",
    "        # Concatenate encoded history and actions\n",
    "        concat = torch.cat([history_repeated, actions_encoded], dim=-1)  # (batch_size, num_actions, hidden_dim*2)\n",
    "\n",
    "        # Score each (history, action) pair\n",
    "        q_values = self.scorer(concat).squeeze(-1)  # (batch_size, num_actions)\n",
    "\n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    MAX_RULES = 10\n",
    "    MAX_ACTIONS = 10\n",
    "    MAX_CHORDS = 10\n",
    "\n",
    "    def __init__(self, chord_sequence):\n",
    "        self.initial_sequence = chord_sequence\n",
    "        self.current_state = chord_sequence.copy()\n",
    "        self.current_nodes = [TreeNode(Chord(chord)) for chord in chord_sequence]\n",
    "        self.actions = self.get_actions()\n",
    "        self.applied_rules = []\n",
    "\n",
    "    # Each index of rhs are a list of applicable rules to currentstate and currentstate + 1\n",
    "    def get_actions(self):\n",
    "        rhs_list = []\n",
    "\n",
    "        for i in range(len(self.current_state) - 1):\n",
    "            chord1 = Chord(self.current_state[i])\n",
    "            chord2 = Chord(self.current_state[i + 1])\n",
    "            possible_rhs = get_possible_rhs_from_str(chord1, chord2)\n",
    "            rhs_list.append(get_applicable_rules(possible_rhs, rules))\n",
    "\n",
    "\n",
    "        return rhs_list\n",
    "\n",
    "    def build_action_index_map(self):\n",
    "\n",
    "        index_map = []\n",
    "        for i, rule_list in enumerate(self.get_actions()):\n",
    "            for j, _ in enumerate(rule_list):\n",
    "                index_map.append((i, j))\n",
    "        return index_map\n",
    "\n",
    "    def apply_rule_index(self,rule_index_i,rule_index_j):\n",
    "        rule = self.actions[rule_index_i][rule_index_j]\n",
    "        node1 = self.current_nodes[rule_index_i]\n",
    "        node2 = self.current_nodes[rule_index_i + 1]\n",
    "        nodes = [node1,node2]\n",
    "        parent_root_index = rule.child_intervals.index('0') \n",
    "        parent = create_parent_node(node1, node2, nodes[parent_root_index], rule.lhs())\n",
    "        self.current_nodes[rule_index_i] = parent\n",
    "        self.current_nodes.pop(rule_index_i + 1)\n",
    "        self.applied_rules.append(rule)\n",
    "        self.current_nodes = get_top_level_nodes(self.current_nodes)\n",
    "        self.current_state = [node.chord.label for node in self.current_nodes]\n",
    "        self.actions = self.get_actions()\n",
    "\n",
    "    def apply_rule(self, rule, node1, node2):\n",
    "        nodes = [node1,node2]\n",
    "        parent_root_index = rule.child_intervals.index('0') \n",
    "        parent = create_parent_node(node1, node2, nodes[parent_root_index], rule.lhs())\n",
    "\n",
    "        self.current_nodes.append(parent)\n",
    "        self.applied_rules.append(rule)\n",
    "\n",
    "        self.current_nodes = get_top_level_nodes(self.current_nodes)\n",
    "        self.current_state = [node.chord.label for node in self.current_nodes]\n",
    "        self.actions = self.get_actions()\n",
    "\n",
    "    def get_state_tensor(self):\n",
    "        QUALITY_VOCAB = ['major', 'minor', 'sus', 'unknown']\n",
    "        INTERVAL_VOCAB = [str(i) for i in range(0,12)]\n",
    "\n",
    "        def one_hot_encode(value, vocab):\n",
    "            vec = [0] * len(vocab)\n",
    "            idx = vocab.index(value if value in vocab else vocab.index('Other'))\n",
    "            vec[idx] = 1\n",
    "            return vec\n",
    "\n",
    "        def encode_rule(rule):\n",
    "            parent_vec = one_hot_encode(rule.parent_quality, QUALITY_VOCAB)\n",
    "\n",
    "            interval1_vec = one_hot_encode(rule.child_intervals[0], INTERVAL_VOCAB)\n",
    "            quality1_vec = one_hot_encode(rule.child_qualities[0], QUALITY_VOCAB)\n",
    "\n",
    "            interval2_vec = one_hot_encode(rule.child_intervals[1], INTERVAL_VOCAB)\n",
    "            quality2_vec = one_hot_encode(rule.child_qualities[1], QUALITY_VOCAB)\n",
    "\n",
    "            return parent_vec + interval1_vec + quality1_vec + interval2_vec + quality2_vec\n",
    "\n",
    "        chord_vector = []\n",
    "        for chord in self.current_state[:self.MAX_CHORDS]:\n",
    "            chord_vector.append(Chord.encode_chord(chord))\n",
    "\n",
    "        # Pad if needed\n",
    "        while len(chord_vector) < self.MAX_CHORDS:\n",
    "            chord_vector.append([0] * len(chord_vector[0]))\n",
    "\n",
    "        flat_chord_vector = [val for chord in chord_vector for val in chord]\n",
    "        chord_tensor = torch.tensor(flat_chord_vector, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        actions = self.get_actions()\n",
    "        action_vector = []\n",
    "\n",
    "        count = 0\n",
    "        for i in range(len(actions)):\n",
    "            for j in range(len(actions[i])):\n",
    "                if count >= self.MAX_ACTIONS:\n",
    "                    break\n",
    "                action_vector.append(encode_rule(actions[i][j]))\n",
    "                count += 1\n",
    "            if count >= self.MAX_ACTIONS:\n",
    "                break\n",
    "\n",
    "        rule_dim = len(action_vector[0]) if action_vector else 0\n",
    "        while len(action_vector) < self.MAX_ACTIONS:\n",
    "            action_vector.append([0] * rule_dim)\n",
    "\n",
    "        actions_tensor = torch.tensor(action_vector, dtype=torch.float32).unsqueeze(0)\n",
    "        return chord_tensor, actions_tensor\n",
    "\n",
    "    def step(self, epsilon=0.1):\n",
    "        chord_tensor, actions_tensor = self.get_state_tensor()\n",
    "        results = self.model.forward(chord_tensor, actions_tensor)\n",
    "        index_map = self.build_action_index_map()[0:10]\n",
    "\n",
    "        if len(index_map) == 0:\n",
    "            return None  # no actions possible\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            flat_index = random.randint(0, len(index_map) - 1)\n",
    "        else:\n",
    "            q_values = self.model(chord_tensor, actions_tensor).detach().squeeze(0)[0:len(index_map)]  # shape: (num_actions,)\n",
    "            flat_index = q_values.squeeze(0).argmax().item()\n",
    "\n",
    "        \n",
    "        i, j = index_map[flat_index]\n",
    "        self.apply_rule_index(i, j)\n",
    "        next_chord_tensor, next_actions_tensor = self.get_state_tensor()\n",
    "        reward = self.evaluate_tree() if self.is_terminal() else -100\n",
    "        done = self.is_terminal()\n",
    "\n",
    "        return {\n",
    "        \"state\": chord_tensor,\n",
    "        \"actions\": actions_tensor,\n",
    "        \"action_index\": flat_index,\n",
    "        \"reward\": reward,\n",
    "        \"next_state\": next_chord_tensor,\n",
    "        \"next_actions\": next_actions_tensor,\n",
    "        \"done\": done\n",
    "    }\n",
    " \n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return len(self.current_state) == 1\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_tree(self):\n",
    "        log_probs = 0\n",
    "        for rule in self.applied_rules:\n",
    "            log_probs += prob_dict.get(rule.make_hashable(), 1e-10)  # Add a small value to avoid log(0)\n",
    "\n",
    "        return log_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chord(root=A#, quality=sus)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chord(\"Bbsus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def train_on_batch(model, optimizer, batch, gamma=0.99):\n",
    "    hist_batch = torch.cat([b[\"state\"] for b in batch])\n",
    "    actions_batch = torch.cat([b[\"actions\"] for b in batch])\n",
    "    next_hist_batch = torch.cat([b[\"next_state\"] for b in batch])\n",
    "    next_actions_batch = torch.cat([b[\"next_actions\"] for b in batch])\n",
    "    \n",
    "    action_indices = torch.tensor([b[\"action_index\"] for b in batch]).unsqueeze(1)\n",
    "    rewards = torch.tensor([b[\"reward\"] for b in batch], dtype=torch.float32)\n",
    "    dones = torch.tensor([b[\"done\"] for b in batch], dtype=torch.bool)\n",
    "\n",
    "    q_values = model(hist_batch, actions_batch)\n",
    "    q_chosen = q_values.gather(1, action_indices).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q_values = model(next_hist_batch, next_actions_batch)  \n",
    "        next_q_max = next_q_values.max(dim=1)[0]\n",
    "        target_q = rewards + gamma * next_q_max * (~dones)\n",
    "\n",
    "    loss = nn.MSELoss()(q_chosen, target_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(f\"\\nBatch size: {len(batch)}\")\n",
    "    # print(\"Sample Q-values:\", q_values[0].tolist())\n",
    "    # print(\"Chosen Q:\", q_chosen[0].item(), \"Target:\", target_q[0].item())\n",
    "    # print(\"Q diff:\", (target_q - q_chosen).abs().mean().item())  # ✅ Add here\n",
    "    # print(\"Loss:\", loss.item())\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, num_episodes=1000, batch_size=32, gamma=0.99):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    buffer = ReplayBuffer(capacity=5000)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Random starting sequence\n",
    "        chord_seq = random.choice(dataset)\n",
    "        env = Environment(chord_seq)\n",
    "        env.add_model(model)\n",
    "        loss = None\n",
    "        while not env.is_terminal():\n",
    "            transition = env.step(epsilon=0.1)\n",
    "            if transition and transition[\"next_actions\"].numel() > 0:\n",
    "                buffer.add(transition)\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                batch = buffer.sample(batch_size)\n",
    "                loss = train_on_batch(model, optimizer, batch, gamma)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            if loss:\n",
    "                print(f\"Episode {episode} complete | Last loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(history_dim=160, action_dim=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "chords_dataset = list(data['chords'])[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 complete | Last loss: 11471.9736\n",
      "Episode 20 complete | Last loss: 2012.8856\n",
      "Episode 30 complete | Last loss: 584.3676\n",
      "Episode 40 complete | Last loss: 680.2872\n",
      "Episode 50 complete | Last loss: 242.5954\n",
      "Episode 60 complete | Last loss: 186.5224\n",
      "Episode 70 complete | Last loss: 1089.6772\n",
      "Episode 80 complete | Last loss: 409.9654\n"
     ]
    }
   ],
   "source": [
    "train_model(model, chords_dataset, num_episodes=1000, batch_size=32, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.add_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(300,36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, history_dim, action_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.history_encoder = nn.Sequential(\n",
    "            nn.Linear(history_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.action_encoder = nn.Sequential(\n",
    "            nn.Linear(action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Output one Q-value\n",
    "        )\n",
    "\n",
    "    def forward(self, history_vec, actions_vecs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            history_vec: Tensor (batch_size, history_dim)\n",
    "            actions_vecs: Tensor (batch_size, num_actions, action_dim)\n",
    "\n",
    "        Returns:\n",
    "            q_values: Tensor (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        batch_size, num_actions, action_dim = actions_vecs.shape\n",
    "\n",
    "        # Encode the history once\n",
    "        history_encoded = self.history_encoder(history_vec)  # (batch_size, hidden_dim)\n",
    "        # Flatten actions and encode them\n",
    "        actions_flat = actions_vecs.view(-1, action_dim)  # (batch_size * num_actions, action_dim)\n",
    "        actions_encoded = self.action_encoder(actions_flat)  # (batch_size * num_actions, hidden_dim)\n",
    "        actions_encoded = actions_encoded.view(batch_size, num_actions, -1)  # (batch_size, num_actions, hidden_dim)\n",
    "\n",
    "        # Repeat history for each action\n",
    "        history_repeated = history_encoded.unsqueeze(1).repeat(1, num_actions, 1)  # (batch_size, num_actions, hidden_dim)\n",
    "\n",
    "        # Concatenate encoded history and actions\n",
    "        concat = torch.cat([history_repeated, actions_encoded], dim=-1)  # (batch_size, num_actions, hidden_dim*2)\n",
    "\n",
    "        # Score each (history, action) pair\n",
    "        q_values = self.scorer(concat).squeeze(-1)  # (batch_size, num_actions)\n",
    "\n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
